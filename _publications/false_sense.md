---
title: "Saliency Maps Give a False Sense of Explanability to Image Classifiers: An Empirical Evaluation across Methods and Metrics"
collection: publications
permalink: /publication/false_sense
excerpt: 'This paper is about an empirical evaluation across saliency methods and corresponding explainable metrics.'
date: 2024-12-07
venue: 'The 16th Asian Conference on Machine Learning (Conference Track)'
paperurl: 'https://raw.githubusercontent.com/mlresearch/v260/main/assets/zhang25a/zhang25a.pdf'
citation: 'Zhang, H., Figueroa, F. T., & Hermanns, H. (2024). Saliency Maps Give a False Sense of Explanability to Image Classifiers: An Empirical Evaluation across Methods and Metrics. In The 16th Asian Conference on Machine Learning (Conference Track).'
---

The interpretability of deep neural networks (DNNs) has emerged as a crucial area of research, particularly in image classification tasks where decisions often lack transparency. Saliency maps have been widely used as a tool to decode the inner workings of these networks by highlighting regions of input images deemed most influential in the classification process. However, recent studies have revealed significant limitations and inconsistencies in the utility of saliency maps as explanations. This paper aims to systematically assess the shortcomings of saliency maps and explore alternative approaches to achieve more reliable and interpretable explanations for image classification models. We carry out a series of experiments to show that 1) the existing evaluation does not provide a fair nor meaningful comparison to the existing saliency maps; these evaluations have their implicit assumption and are not differentiable; 2) the saliency maps do not provide enough information on explaining the accuracy of network, the relationship between classes and the modification of the images.

[paper](https://raw.githubusercontent.com/mlresearch/v260/main/assets/zhang25a/zhang25a.pdf)
